# prometheus-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: observability
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      external_labels:
        cluster: 'asia-southeast1'
        replica: '$(POD_NAME)'
    
    rule_files:
      - /etc/prometheus/rules.yml
    
    alerting:
      alertmanagers:
        - static_configs:
            - targets:
                - alertmanager.observability.svc.cluster.local:9093
          api_version: v2
         
    scrape_configs:
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
          - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: default;kubernetes;https

      - job_name: 'kubernetes-nodes'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics

      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name
      
      - job_name: 'kube-state-metrics'
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]
            regex: kube-state-metrics
            action: keep
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: kubernetes_name
          
      - job_name: 'node-exporter'
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]
            regex: node-exporter
            action: keep
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: kubernetes_name
            

  rules.yml: |
    groups: 
      - name: kubernetes-apps
        rules:
          - alert: KubePodCrashLooping
            expr: max_over_time(kube_pod_container_status_waiting_reason{kubernetes_pod_name="kube-state-metrics-0",namespace=~".*",reason="CrashLoopBackOff"}[5m]) >= 1
            for: 15m
            labels:
              severity: warning
            annotations:
              summary: Pod is crash looping
            
          - alert: KubePodNotReady
            expr: sum by(namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{job="kube-state-metrics",namespace=~".*",phase=~"Pending|Unknown"}) * on(namespace, pod) group_left(owner_kind) topk by(namespace, pod) (1, max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"}))) > 0
            for: 15m
            labels:
              severity: warning
            annotations:
              summary: Pod has been in a non-ready state for more than 15 minutes
              description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes.
          
          - alert: KubeDeploymentReplicasMismatch
            expr: kube_deployment_spec_replicas{kubernetes_pod_name="kube-state-metrics-0", namespace=~".*"} > kube_deployment_status_replicas_available{kubernetes_pod_name="kube-state-metrics-0", namespace=~".*"} and (changes(kube_deployment_status_replicas_updated{kubernetes_pod_name="kube-state-metrics-0", namespace=~".*"}[10m]) == 0)
            for: 15m
            labels:
              severity: warning
            annotations:
              summary: 디플로이먼트가 예상된 레플리카 수에 도달하지 못함
              description: "{{ $labels.namespace }} 네임스페이스의 {{ $labels.pod }} 파드가 예상된 레플리카 수에 도달하지 못했습니다."
          - alert: KubeContainerWaiting
            expr: sum by(namespace, pod, container) (kube_pod_container_status_waiting_reason{kubernetes_pod_name="kube-state-metrics-0", namespace=~".*"}) > 0
            for: 1h
            labels:
              severity: warning
            annotations:
              description: "{{ $labels.namespace }} 네임스페이스의 {{ $labels.pod }} 파드에 있는 {{ $labels.container }} 컨테이너가 1시간 이상 대기 상태에 있습니다."
              runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontainerwaiting
              summary: "파드 컨테이너가 1시간 이상 대기 중"
      - name: kubernetes-storage
        rules:
          - alert: KubePersistentVolumeErrors
            expr: kube_persistentvolumeclaim_status_phase{kubernetes_pod_name="kube-state-metrics-0", phase=~"Failed|Pending"} > 0
            for: 5m
            labels:
              severity: critical
            annotations:
                summary: PersistentVolume is having issues with provisioning.
                description: PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.pod }} is in a failed state.
                
      - name: kubernetes-system-kubelet
        rules:
        alert: KubeletClientCertificateExpiration
        expr: kubelet_certificate_manager_client_ttl_seconds < 604800
        labels:
          severity: warning
        annotations:
        description: Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
        summary: Kubelet client certificate is about to expire.
        
      - name: node-exporter
        rules:
          - alert: HostOutOfMemory
            expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host out of memory (instance {{ $labels.instance }})
              description: "Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          
          - alert: HostHighCPULoad
            expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 80
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Host high CPU load (instance {{ $labels.instance }})
              description: "CPU load is > 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          
          - alert: HostOutOfDiskSpace
            expr: (node_filesystem_avail_bytes{mountpoint="/"}  * 100) / node_filesystem_size_bytes{mountpoint="/"} < 10
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: Host out of disk space (instance {{ $labels.instance }})
              description: "Disk space is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - name: alertmanager
        rules:
          - alert: AlertmanagerFailedReload
            expr: max_over_time(alertmanager_config_last_reload_successful{app="alertmanager"}[5m]) == 0
            for: 10m
            labels:
              severity: critical
            annotations:
              summary: Reloading an Alertmanager configuration has failed.
              description: Configuration has failed to load for {{ $labels.namespace }}/{{ $labels.pod}}.
          
          - alert: AlertmanagerFailedToSendAlerts
            expr: (rate(alertmanager_notifications_failed_total{app="-alertmanager"}[5m]) / rate(alertmanager_notifications_total{app="alertmanager"}[5m])) > 0.01
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: An Alertmanager instance failed to send notifications
              description: Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} failed to send {{ $value | humanizePercentage }} of notifications to {{ $labels.integration }}.
      
      - name: general
        rules:
         - alert: Watch
           expr: vector(1)
           labels:
              severity: none
           annotations:
              summary: "This is a general-purpose alert."
              description: "This alert is for testing purposes."
         
         - alerts: InfoInhibitor
           expr: ALERTS{severity="info"} == 1 unless on(namespace) ALERTS{alertname!="InfoInhibitor",alertstate="firing",severity=~"warning|critical"} == 1
           labels:
              severity: none
              annotations:
                summary: "정보 수준 알림 억제 메커니즘"
                description: >
                  이 알림은 정보(info) 수준 알림을 관리하기 위해 사용됩니다. 
                  정보 수준 알림은 때로 너무 빈번하게 발생하여 불필요한 소음이 될 수 있지만, 
                  다른 중요한 알림과 함께 발생할 때는 유용한 정보가 됩니다. 
                  이 알림은 정보 수준 알림이 발생할 때 활성화되며, 
                  같은 네임스페이스에서 '경고' 또는 '심각' 수준의 다른 알림이 발생하면 비활성화됩니다.         
            
      - name: config-reloaders
        rules:
          - alert: ReloaderFailure
            expr: max_over_time(reloader_last_reload_successful{kubernetes_namespace=~".+"}[5m]) == 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Reloader 실패 감지"
              description: "{{ $labels.app }} 애플리케이션의 reloader가 지난 5분 동안 성공적으로 리로드하지 못했습니다."
     
      - name: kube-prometheus-general
        records: 
        - record: count:up1
          expr: count without(instance, pod, node) (up == 1)
        
        - record: count:up0
          expr: count without(instance, pod, node) (up == 1)
     
      - name: kube-prometheus-node-recording
        records:
          - record: instance:node_cpu:rate:sum
            expr: count without(cpu, mode) (node_cpu_seconds_total)
     
      - name: prometheus
        rules:
          - alert: PrometheusBadConfig
            expr: max_over_time(prometheus_config_last_reload_successful{job="datacenter-kube-prometheus-prometheus",namespace="datacenter"}[5m]) == 0
            for: 10m
            labels:
              severity: critical
            annotations:
              summary: Failed Prometheus configuration reload
              description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to reload its configuration.
          
          - alert: PrometheusNotIngestingSamples
            expr: (rate(prometheus_tsdb_head_samples_appended_total{job="datacenter-kube-prometheus-prometheus",namespace="datacenter"}[5m]) <= 0 and (sum without(scrape_job) (prometheus_target_metadata_cache_entries{job="datacenter-kube-prometheus-prometheus",namespace="datacenter"}) > 0 or sum without(rule_group) (prometheus_rule_group_rules{job="datacenter-kube-prometheus-prometheus",namespace="datacenter"}) > 0))
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: Prometheus is not ingesting samples
              description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting samples.
              
